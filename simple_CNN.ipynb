{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stains: 870\n",
      "Maximum number of stains in a single image: 14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "annotation_dir = os.path.join('fabric_stain_dataset', 'annotations', 'stain')\n",
    "total_stains = 0\n",
    "max_stains = 0\n",
    "\n",
    "for annotation_file in os.listdir(annotation_dir):\n",
    "    annotation_file = os.path.join(annotation_dir, annotation_file)\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        num_stains = len(file.readlines())\n",
    "        total_stains += num_stains\n",
    "        if num_stains > max_stains:\n",
    "            max_stains = num_stains\n",
    "\n",
    "print(\"Total number of stains:\", total_stains)\n",
    "print(\"Maximum number of stains in a single image:\", max_stains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved 319 images with 696 stains to fabric_stain_dataset/split_images/train\n",
      "Moved 79 images with 174 to fabric_stain_dataset/split_images/val\n",
      "Train val split is 80.00% - 20.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "#Because there are multiple stains per iamge, this gets a bit tricky\n",
    "source_dir = os.path.join('fabric_stain_dataset', 'images', 'stain')\n",
    "train_dir = os.path.join('fabric_stain_dataset', 'split_images', 'train')\n",
    "val_dir = os.path.join('fabric_stain_dataset', 'split_images', 'val')\n",
    "train_annotation_dir = os.path.join('fabric_stain_dataset', 'split_annotations', 'train')\n",
    "val_annotation_dir = os.path.join('fabric_stain_dataset', 'split_annotations', 'val')\n",
    "annotation_dir = os.path.join('fabric_stain_dataset', 'annotations', 'stain')\n",
    "\n",
    "# Create the balanced dataset folders if it doesn't exist\n",
    "def handle_dir_creation(dir):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "    else:\n",
    "        # Remove all the files in the folder\n",
    "        for file_name in os.listdir(dir):\n",
    "            file_path = os.path.join(dir, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    os.unlink(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "# Create the directories if they don't exist\n",
    "handle_dir_creation(train_dir)\n",
    "handle_dir_creation(val_dir)\n",
    "handle_dir_creation(train_annotation_dir)\n",
    "handle_dir_creation(val_annotation_dir)\n",
    "\n",
    "# Ensure the target directories exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "os.makedirs(train_annotation_dir, exist_ok=True)\n",
    "os.makedirs(val_annotation_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#We'll be using an 80-20 split for the training and validating datasets. this is pretty standard.\n",
    "train_test_split = 0.8 \n",
    "\n",
    "image_files = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "\n",
    "# Shuffle the list to ensure randomness\n",
    "np.random.shuffle(image_files)\n",
    "\n",
    "#Find the total number of stains in the dataset\n",
    "total_stains = 0\n",
    "for annotation_file in os.listdir(annotation_dir):\n",
    "    annotation_file = os.path.join(annotation_dir, annotation_file)\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        num_stains = len(file.readlines())\n",
    "        total_stains += num_stains\n",
    "\n",
    "num_train_stains = int(total_stains * 0.8)\n",
    "current_train_stains = 0\n",
    "\n",
    "\n",
    "for img in image_files:\n",
    "    # Get the annotation file for the image\n",
    "    annotation_file = os.path.join(annotation_dir, img.replace('.jpg', '.txt'))\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        num_stains = len(file.readlines())\n",
    "        if current_train_stains + num_stains <= num_train_stains:\n",
    "            current_train_stains += num_stains\n",
    "            shutil.copy(os.path.join(source_dir, img), os.path.join(train_dir, img))\n",
    "            shutil.copy(annotation_file, os.path.join(train_annotation_dir, os.path.basename(annotation_file)))\n",
    "        else:\n",
    "            shutil.copy(os.path.join(source_dir, img), os.path.join(val_dir, img))\n",
    "            shutil.copy(annotation_file, os.path.join(val_annotation_dir, os.path.basename(annotation_file)))\n",
    "\n",
    "num_train_imgs = len(os.listdir(train_dir))\n",
    "num_val_imgs = len(os.listdir(val_dir))\n",
    "\n",
    "print(f\"Moved {num_train_imgs} images with {current_train_stains} stains to {train_dir}\")\n",
    "print(f\"Moved {num_val_imgs} images with {total_stains - current_train_stains} to {val_dir}\")\n",
    "print(f\"Train val split is {current_train_stains / total_stains * 100:.2f}% - {(total_stains - current_train_stains) / total_stains * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Directories\n",
    "train_dir = os.path.join('fabric_stain_dataset', 'split_images', 'train')\n",
    "val_dir = os.path.join('fabric_stain_dataset', 'split_images', 'val')\n",
    "train_annotation_dir = os.path.join('fabric_stain_dataset', 'split_annotations', 'train')\n",
    "val_annotation_dir = os.path.join('fabric_stain_dataset', 'split_annotations', 'val')\n",
    "\n",
    "# Function to load annotations\n",
    "def load_annotations(annotation_file):\n",
    "    annotations = []\n",
    "    with open(annotation_file, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            label = int(parts[0])\n",
    "            x_center = float(parts[1])\n",
    "            y_center = float(parts[2])\n",
    "            width = float(parts[3])\n",
    "            height = float(parts[4])\n",
    "            annotations.append([label, x_center, y_center, width, height])\n",
    "    return np.array(annotations)\n",
    "\n",
    "# Function to preprocess data\n",
    "def preprocess_data(image_dir, annotation_dir, image_size=(32, 32)):\n",
    "    data = []\n",
    "    annotations = []\n",
    "    max_annotations = 0\n",
    "    for img_name in os.listdir(image_dir):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            image_path = os.path.join(image_dir, img_name)\n",
    "            annotation_path = os.path.join(annotation_dir, img_name.replace('.jpg', '.txt'))\n",
    "            \n",
    "            if os.path.isfile(annotation_path):\n",
    "                image = Image.open(image_path)\n",
    "                image = image.resize(image_size)  # Resize image\n",
    "                image = image.convert('L')  # Convert to grayscale\n",
    "                image = np.array(image) / 255.0  # Normalize the image\n",
    "                image = np.expand_dims(image, axis=-1)  # Add channel dimension\n",
    "                annotation = load_annotations(annotation_path)\n",
    "                \n",
    "                data.append(image)\n",
    "                annotations.append(annotation)\n",
    "                \n",
    "                # Update the maximum number of annotations\n",
    "                if annotation.shape[0] > max_annotations:\n",
    "                    max_annotations = annotation.shape[0]\n",
    "    \n",
    "    return np.array(data), annotations, max_annotations\n",
    "\n",
    "# Preprocess training and validation data\n",
    "train_data, train_annotations, train_max_annotations = preprocess_data(train_dir, train_annotation_dir)\n",
    "val_data, val_annotations, val_max_annotations = preprocess_data(val_dir, val_annotation_dir)\n",
    "\n",
    "# Determine the global maximum number of annotations\n",
    "global_max_annotations = max(train_max_annotations, val_max_annotations)\n",
    "\n",
    "# Pad annotations to the global maximum length\n",
    "def pad_annotations(annotations, max_annotations):\n",
    "    padded_annotations = []\n",
    "    for annotation in annotations:\n",
    "        if annotation.shape[0] < max_annotations:\n",
    "            padding = np.zeros((max_annotations - annotation.shape[0], annotation.shape[1]))\n",
    "            padded_annotation = np.vstack((annotation, padding))\n",
    "        else:\n",
    "            padded_annotation = annotation\n",
    "        padded_annotations.append(padded_annotation)\n",
    "    return np.array(padded_annotations)\n",
    "\n",
    "train_annotations_padded = pad_annotations(train_annotations, global_max_annotations)\n",
    "val_annotations_padded = pad_annotations(val_annotations, global_max_annotations)\n",
    "\n",
    "# Flatten the annotations to match the model output\n",
    "train_annotations_flat = train_annotations_padded.reshape((train_annotations_padded.shape[0], -1))\n",
    "val_annotations_flat = val_annotations_padded.reshape((val_annotations_padded.shape[0], -1))\n",
    "\n",
    "print(\"Shape of train_annotations_flat:\", train_annotations_flat.shape)\n",
    "print(\"Shape of val_annotations_flat:\", val_annotations_flat.shape)\n",
    "\n",
    "print(\"Data preprocessing completed\")\n",
    "\n",
    "# Define a more suitable model for handling bounding boxes\n",
    "print(\"Building the model\")\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(train_annotations_flat.shape[1], activation='sigmoid'))\n",
    "print(\"Model built\")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error',  # Use appropriate loss function\n",
    "              metrics=['accuracy'])\n",
    "print(\"Model compiled\")\n",
    "\n",
    "# Print the model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Fit the model using the augmented data generator\n",
    "print(\"Training the model with data augmentation\")\n",
    "history = model.fit(datagen.flow(train_data, train_annotations_flat, batch_size=32),\n",
    "                    steps_per_epoch=len(train_data) // 32,\n",
    "                    epochs=9,\n",
    "                    validation_data=(val_data, val_annotations_flat))\n",
    "\n",
    "# Print the training metrics\n",
    "print(\"Training Metrics:\")\n",
    "for epoch in range(len(history.history['loss'])):\n",
    "    print(f\"Epoch {epoch + 1}:\")\n",
    "    print(f\"  Loss: {history.history['loss'][epoch]}\")\n",
    "    print(f\"  Accuracy: {history.history['accuracy'][epoch]}\")\n",
    "    print(f\"  Validation Loss: {history.history['val_loss'][epoch]}\")\n",
    "    print(f\"  Validation Accuracy: {history.history['val_accuracy'][epoch]}\")\n",
    "\n",
    "# Evaluate the model on the validation data\n",
    "val_loss, val_accuracy = model.evaluate(val_data, val_annotations_flat, verbose=2)\n",
    "\n",
    "# Print the validation metrics\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Predict on validation data\n",
    "val_predictions = model.predict(val_data)\n",
    "\n",
    "# Calculate Intersection over Union (IoU)\n",
    "def calculate_iou(true_boxes, pred_boxes):\n",
    "    ious = []\n",
    "    for true_box, pred_box in zip(true_boxes, pred_boxes):\n",
    "        _, x1_true, y1_true, w_true, h_true = true_box  # Unpack the true box\n",
    "        _, x1_pred, y1_pred, w_pred, h_pred = pred_box  # Unpack the predicted box\n",
    "        \n",
    "        x1_true, y1_true = x1_true - w_true / 2, y1_true - h_true / 2\n",
    "        x2_true, y2_true = x1_true + w_true, y1_true + h_true\n",
    "        x1_pred, y1_pred = x1_pred - w_pred / 2, y1_pred - h_pred / 2\n",
    "        x2_pred, y2_pred = x1_pred + w_pred, y1_pred + h_pred\n",
    "        \n",
    "        xi1, yi1 = max(x1_true, x1_pred), max(y1_true, y1_pred)\n",
    "        xi2, yi2 = min(x2_true, x2_pred), min(y2_true, y2_pred)\n",
    "        \n",
    "        inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "        true_area = (x2_true - x1_true) * (y2_true - y1_true)\n",
    "        pred_area = (x2_pred - x1_pred) * (y2_pred - y1_pred)\n",
    "        \n",
    "        union_area = true_area + pred_area - inter_area\n",
    "        iou = inter_area / union_area if union_area != 0 else 0\n",
    "        ious.append(iou)\n",
    "    return np.mean(ious)\n",
    "\n",
    "# Calculate IoU for validation data\n",
    "ious = []\n",
    "for i in range(len(val_annotations_flat)):\n",
    "    true_boxes = val_annotations_flat[i].reshape(-1, 5)\n",
    "    pred_boxes = val_predictions[i].reshape(-1, 5)\n",
    "    iou = calculate_iou(true_boxes, pred_boxes)\n",
    "    ious.append(iou)\n",
    "\n",
    "mean_iou = np.mean(ious)\n",
    "print(f\"Mean IoU: {mean_iou}\")\n",
    "\n",
    "# Save the model\n",
    "model.save('simple_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results taken from Google Collab\n",
    "\n",
    "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
    "Shape of train_annotations_flat: (319, 70)\n",
    "Shape of val_annotations_flat: (79, 70)\n",
    "Data preprocessing completed\n",
    "Building the model\n",
    "Model built\n",
    "Model compiled\n",
    "Model Summary:\n",
    "Model: \"sequential_13\"\n",
    "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
    "┃ Layer (type)                         ┃ Output Shape                ┃         Param # ┃\n",
    "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
    "│ conv2d_39 (Conv2D)                   │ (None, 30, 30, 32)          │             320 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ batch_normalization_39               │ (None, 30, 30, 32)          │             128 │\n",
    "│ (BatchNormalization)                 │                             │                 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ max_pooling2d_39 (MaxPooling2D)      │ (None, 15, 15, 32)          │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ conv2d_40 (Conv2D)                   │ (None, 13, 13, 64)          │          18,496 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ batch_normalization_40               │ (None, 13, 13, 64)          │             256 │\n",
    "│ (BatchNormalization)                 │                             │                 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ max_pooling2d_40 (MaxPooling2D)      │ (None, 6, 6, 64)            │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ conv2d_41 (Conv2D)                   │ (None, 4, 4, 128)           │          73,856 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ batch_normalization_41               │ (None, 4, 4, 128)           │             512 │\n",
    "│ (BatchNormalization)                 │                             │                 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ max_pooling2d_41 (MaxPooling2D)      │ (None, 2, 2, 128)           │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ flatten_13 (Flatten)                 │ (None, 512)                 │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_32 (Dense)                     │ (None, 512)                 │         262,656 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dropout_19 (Dropout)                 │ (None, 512)                 │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_33 (Dense)                     │ (None, 256)                 │         131,328 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dropout_20 (Dropout)                 │ (None, 256)                 │               0 │\n",
    "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
    "│ dense_34 (Dense)                     │ (None, 70)                  │          17,990 │\n",
    "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
    " Total params: 505,542 (1.93 MB)\n",
    " Trainable params: 505,094 (1.93 MB)\n",
    " Non-trainable params: 448 (1.75 KB)\n",
    "Training the model with data augmentation\n",
    "Epoch 1/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 4s 99ms/step - accuracy: 0.0672 - loss: 0.2106 - val_accuracy: 0.2278 - val_loss: 0.1812\n",
    "Epoch 2/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.0938 - loss: 0.0653 - val_accuracy: 0.2278 - val_loss: 0.1724\n",
    "Epoch 3/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 1s 91ms/step - accuracy: 0.1820 - loss: 0.0450 - val_accuracy: 0.1772 - val_loss: 0.0970\n",
    "Epoch 4/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - accuracy: 0.0938 - loss: 0.0233 - val_accuracy: 0.2278 - val_loss: 0.0907\n",
    "Epoch 5/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 1s 114ms/step - accuracy: 0.1809 - loss: 0.0250 - val_accuracy: 0.2658 - val_loss: 0.0528\n",
    "Epoch 6/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 23ms/step - accuracy: 0.2500 - loss: 0.0196 - val_accuracy: 0.2658 - val_loss: 0.0499\n",
    "Epoch 7/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 2s 75ms/step - accuracy: 0.2083 - loss: 0.0234 - val_accuracy: 0.2658 - val_loss: 0.0315\n",
    "Epoch 8/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.4375 - loss: 0.0179 - val_accuracy: 0.2658 - val_loss: 0.0301\n",
    "Epoch 9/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 1s 71ms/step - accuracy: 0.2604 - loss: 0.0225 - val_accuracy: 0.3165 - val_loss: 0.0223\n",
    "Epoch 10/10\n",
    "9/9 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step - accuracy: 0.1250 - loss: 0.0221 - val_accuracy: 0.2785 - val_loss: 0.0218\n",
    "Training Metrics:\n",
    "Epoch 1:\n",
    "  Loss: 0.15669475495815277\n",
    "  Accuracy: 0.08013937622308731\n",
    "  Validation Loss: 0.18115410208702087\n",
    "  Validation Accuracy: 0.2278480976819992\n",
    "Epoch 2:\n",
    "  Loss: 0.06529563665390015\n",
    "  Accuracy: 0.09375\n",
    "  Validation Loss: 0.17244872450828552\n",
    "  Validation Accuracy: 0.2278480976819992\n",
    "Epoch 3:\n",
    "  Loss: 0.037273991852998734\n",
    "  Accuracy: 0.16724738478660583\n",
    "  Validation Loss: 0.09699070453643799\n",
    "  Validation Accuracy: 0.17721518874168396\n",
    "Epoch 4:\n",
    "  Loss: 0.02330152317881584\n",
    "  Accuracy: 0.09375\n",
    "  Validation Loss: 0.09067019820213318\n",
    "  Validation Accuracy: 0.2278480976819992\n",
    "Epoch 5:\n",
    "  Loss: 0.02430378459393978\n",
    "  Accuracy: 0.20209059119224548\n",
    "  Validation Loss: 0.05283856764435768\n",
    "  Validation Accuracy: 0.26582279801368713\n",
    "Epoch 6:\n",
    "  Loss: 0.019597254693508148\n",
    "  Accuracy: 0.25\n",
    "  Validation Loss: 0.049933385103940964\n",
    "  Validation Accuracy: 0.26582279801368713\n",
    "Epoch 7:\n",
    "  Loss: 0.023313453420996666\n",
    "  Accuracy: 0.24041812121868134\n",
    "  Validation Loss: 0.03150640428066254\n",
    "  Validation Accuracy: 0.26582279801368713\n",
    "Epoch 8:\n",
    "  Loss: 0.017877992242574692\n",
    "  Accuracy: 0.4375\n",
    "  Validation Loss: 0.030076123774051666\n",
    "  Validation Accuracy: 0.26582279801368713\n",
    "Epoch 9:\n",
    "  Loss: 0.022600959986448288\n",
    "  Accuracy: 0.24738675355911255\n",
    "  Validation Loss: 0.022342901676893234\n",
    "  Validation Accuracy: 0.3164556920528412\n",
    "Epoch 10:\n",
    "  Loss: 0.02209358662366867\n",
    "  Accuracy: 0.125\n",
    "  Validation Loss: 0.021788330748677254\n",
    "  Validation Accuracy: 0.27848100662231445\n",
    "3/3 - 0s - 18ms/step - accuracy: 0.2785 - loss: 0.0218\n",
    "Validation Loss: 0.021788330748677254\n",
    "Validation Accuracy: 0.27848100662231445\n",
    "3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 76ms/step\n",
    "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
    "Mean IoU: 0.0037942546209400346"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
